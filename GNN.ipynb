{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3b785e3-3434-4e23-90e1-646af523cc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# print(f\"PyTorch version: {torch.__version__}\")\n",
    "# print(f\"CUDA version: {torch.version.cuda}\")\n",
    "# print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# # Xóa các thư viện cũ để tránh xung đột\n",
    "# !pip uninstall -y torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric\n",
    "\n",
    "# # Cài đặt torch-scatter\n",
    "# !pip install torch-scatter -f https://data.pyg.org/whl/torch-2.5.1+cu124.html\n",
    "\n",
    "# # Cài đặt torch-sparse\n",
    "# !pip install torch-sparse -f https://data.pyg.org/whl/torch-2.5.1+cu124.html\n",
    "\n",
    "# # Cài đặt torch-cluster\n",
    "# !pip install torch-cluster -f https://data.pyg.org/whl/torch-2.5.1+cu124.html\n",
    "\n",
    "# # Cài đặt torch-spline-conv\n",
    "# !pip install torch-spline-conv -f https://data.pyg.org/whl/torch-2.5.1+cu124.html\n",
    "# # Cài đặt torch-geometric\n",
    "# !pip install torch-geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "174823f9-9ff7-4c58-98be-d4377deb1fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import NeighborSampler\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import gc\n",
    "import os\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c57a04ad-6a35-4214-876c-7d1df1cdf5f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU Memory: 2.15 GB\n",
      "GPU Name: NVIDIA GeForce MX450\n"
     ]
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Clear memory\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Kiểm tra GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d31f856-d797-436b-8d31-51b2f8c3b49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Đọc dữ liệu từ Kaggle\n",
    "train_df = pd.read_csv('data2/train.csv')\n",
    "test_df = pd.read_csv('data2/test.csv')\n",
    "\n",
    "# Feature engineering: Add Price * Super Area\n",
    "train_df['Price_x_SuperArea'] = train_df['Price'] * train_df['Super Area']\n",
    "test_df['Price_x_SuperArea'] = test_df['Price'] * test_df['Super Area']\n",
    "\n",
    "# Thêm cột index làm house_id\n",
    "train_df['index'] = train_df.index\n",
    "test_df['index'] = test_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b1aa1d3-5752-44f1-b1e9-4118156657f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm để gán node vào lá của một cây (độ sâu 3)\n",
    "def assign_to_leaf(df):\n",
    "    leaf_ids = np.zeros(len(df), dtype=np.int32)\n",
    "    for idx in range(len(df)):\n",
    "        price = df.iloc[idx]['Price']\n",
    "        super_area = df.iloc[idx]['Super Area']\n",
    "        carpet_area = df.iloc[idx]['Carpet Area']\n",
    "        total_floors = df.iloc[idx]['Total Floors']\n",
    "        \n",
    "        if price <= 10308.50:\n",
    "            if super_area <= 1800.04:\n",
    "                if carpet_area <= 1915.50:\n",
    "                    if price <= 5056.50:\n",
    "                        leaf_ids[idx] = 0\n",
    "                    else:\n",
    "                        leaf_ids[idx] = 1\n",
    "                else:\n",
    "                    if total_floors <= 4.50:\n",
    "                        leaf_ids[idx] = 2\n",
    "                    else:\n",
    "                        leaf_ids[idx] = 3\n",
    "            else:\n",
    "                if price <= 6952.00:\n",
    "                    if super_area <= 3375.00:\n",
    "                        leaf_ids[idx] = 4\n",
    "                    else:\n",
    "                        leaf_ids[idx] = 5\n",
    "                else:\n",
    "                    if super_area <= 3331.01:\n",
    "                        leaf_ids[idx] = 6\n",
    "                    else:\n",
    "                        leaf_ids[idx] = 7\n",
    "        else:\n",
    "            if super_area <= 2592.52:\n",
    "                if price <= 18905.50:\n",
    "                    if super_area <= 1450.00:\n",
    "                        leaf_ids[idx] = 8\n",
    "                    else:\n",
    "                        leaf_ids[idx] = 9\n",
    "                else:\n",
    "                    if super_area <= 1150.00:\n",
    "                        leaf_ids[idx] = 10\n",
    "                    else:\n",
    "                        leaf_ids[idx] = 11\n",
    "            else:\n",
    "                if price <= 29375.00:\n",
    "                    if price <= 15997.50:\n",
    "                        leaf_ids[idx] = 12\n",
    "                    else:\n",
    "                        leaf_ids[idx] = 13\n",
    "                else:\n",
    "                    if super_area <= 3924.96:\n",
    "                        leaf_ids[idx] = 14\n",
    "                    else:\n",
    "                        leaf_ids[idx] = 15\n",
    "    return leaf_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "169843a9-9232-4f90-9b64-4c5aac55551d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm tiền xử lý dữ liệu\n",
    "def preprocess_data(df, numerical_cols, categorical_cols, target_col, \n",
    "                    scaler=None, encoders=None, target_encoders=None, target_scaler=None, is_train=True):\n",
    "    df = df.copy()\n",
    "    \n",
    "    target_encode_cols = ['Society', 'Location', 'Overlooking']\n",
    "    \n",
    "    if is_train:\n",
    "        scaler = MinMaxScaler()\n",
    "        df[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n",
    "        \n",
    "        encoders = {}\n",
    "        target_encoders = {}\n",
    "        target_scaler = MinMaxScaler()\n",
    "        \n",
    "        encoded_features = []\n",
    "        one_hot_cols = ['Transaction', 'Furnishing', 'Ownership', 'Facing']\n",
    "        for col in one_hot_cols:\n",
    "            encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "            encoded = encoder.fit_transform(df[[col]])\n",
    "            encoded_df = pd.DataFrame(\n",
    "                encoded, \n",
    "                columns=[f\"{col}_{cat}\" for cat in encoder.categories_[0]]\n",
    "            )\n",
    "            encoded_features.append(encoded_df)\n",
    "            encoders[col] = encoder\n",
    "        \n",
    "        target_encoded = []\n",
    "        for col in target_encode_cols:\n",
    "            mean_target = df.groupby(col)[target_col].mean()\n",
    "            df[f'{col}_encoded'] = df[col].map(mean_target)\n",
    "            target_encoders[col] = mean_target\n",
    "            target_encoded.append(df[[f'{col}_encoded']])\n",
    "        \n",
    "        target_encoded_df = pd.concat(target_encoded, axis=1)\n",
    "        scaled_target_encoded = target_scaler.fit_transform(target_encoded_df)\n",
    "        for i, col in enumerate(target_encode_cols):\n",
    "            df[f'{col}_encoded'] = scaled_target_encoded[:, i]\n",
    "            encoded_features.append(df[[f'{col}_encoded']])\n",
    "    else:\n",
    "        if scaler is None or encoders is None or target_encoders is None or target_scaler is None:\n",
    "            raise ValueError(\"Scaler, encoders, target_encoders, and target_scaler must be provided for is_train=False\")\n",
    "        \n",
    "        df[numerical_cols] = scaler.transform(df[numerical_cols])\n",
    "        \n",
    "        encoded_features = []\n",
    "        for col in ['Transaction', 'Furnishing', 'Ownership', 'Facing']:\n",
    "            encoded = encoders[col].transform(df[[col]])\n",
    "            encoded_df = pd.DataFrame(\n",
    "                encoded, \n",
    "                columns=[f\"{col}_{cat}\" for cat in encoders[col].categories_[0]]\n",
    "            )\n",
    "            encoded_features.append(encoded_df)\n",
    "        \n",
    "        target_encoded = []\n",
    "        for col in target_encode_cols:\n",
    "            default_value = df[target_col].mean() if target_col in df else 0\n",
    "            if target_encoders and col in target_encoders:\n",
    "                df[f'{col}_encoded'] = df[col].map(target_encoders[col]).fillna(default_value)\n",
    "            else:\n",
    "                df[f'{col}_encoded'] = default_value\n",
    "            target_encoded.append(df[[f'{col}_encoded']])\n",
    "        \n",
    "        target_encoded_df = pd.concat(target_encoded, axis=1)\n",
    "        scaled_target_encoded = target_scaler.transform(target_encoded_df)\n",
    "        for i, col in enumerate(target_encode_cols):\n",
    "            df[f'{col}_encoded'] = scaled_target_encoded[:, i]\n",
    "            encoded_features.append(df[[f'{col}_encoded']])\n",
    "    \n",
    "    feature_df = pd.concat([df[numerical_cols]] + encoded_features, axis=1)\n",
    "    y = np.log1p(df[target_col].values) if target_col in df else None\n",
    "    \n",
    "    return feature_df, y, scaler, encoders, target_encoders, target_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5be1c7af-e1fe-461d-9e1f-5a3c36483abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess train data\n",
    "numerical_cols = ['Carpet Area', 'Super Area', 'Bathroom', 'Balcony', 'Current Floor', \n",
    "                  'Total Floors', 'BHK', 'Price', 'Car Parking', 'Price_x_SuperArea']\n",
    "categorical_cols = ['Transaction', 'Furnishing', 'Overlooking', 'Ownership', 'Facing']\n",
    "target_col = 'Amount'\n",
    "\n",
    "features, y, scaler, encoders, target_encoders, target_scaler = preprocess_data(\n",
    "    train_df, numerical_cols, categorical_cols, target_col, is_train=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa7d7b84-bf4c-411e-8d89-4dc7a4af89e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating SIMILAR_LEAF edges from Random Forest structure...\n"
     ]
    }
   ],
   "source": [
    "# Tạo SIMILAR_LEAF edges từ cấu trúc cây (Tối ưu hơn)\n",
    "print(\"Creating SIMILAR_LEAF edges from Random Forest structure...\")\n",
    "leaf_indices = np.zeros((len(train_df), 3), dtype=np.int32)\n",
    "for tree_idx in range(3):\n",
    "    leaf_indices[:, tree_idx] = assign_to_leaf(train_df)\n",
    "\n",
    "# Nhóm các node theo lá\n",
    "leaf_groups = defaultdict(list)\n",
    "for idx in range(len(train_df)):\n",
    "    leaf_tuple = tuple(leaf_indices[idx])\n",
    "    leaf_groups[leaf_tuple].append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5a6277a-ed7d-4ce1-90ae-25f90ff59fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo file tạm để lưu trữ edges và weights\n",
    "edges_file = 'store/similar_leaf_edges.npy'\n",
    "weights_file = 'store/similar_leaf_weights.npy'\n",
    "if os.path.exists(edges_file):\n",
    "    os.remove(edges_file)\n",
    "if os.path.exists(weights_file):\n",
    "    os.remove(weights_file)\n",
    "\n",
    "# Biến đếm số lượng cạnh\n",
    "total_edges = 0\n",
    "amounts = train_df['Amount'].values\n",
    "batch_size = 1000  # Xử lý theo batch để giảm bộ nhớ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28dbb920-ece2-4988-814b-85ddddfc3918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing leaf group with 2298 nodes...\n",
      "Processing leaf group with 10087 nodes...\n",
      "Processing leaf group with 1917 nodes...\n",
      "Processing leaf group with 12382 nodes...\n",
      "Processing leaf group with 3027 nodes...\n",
      "Processing leaf group with 566 nodes...\n",
      "Processing leaf group with 1883 nodes...\n",
      "Processing leaf group with 302 nodes...\n",
      "Processing leaf group with 541 nodes...\n",
      "Processing leaf group with 411 nodes...\n",
      "Processing leaf group with 170 nodes...\n",
      "Processing leaf group with 124 nodes...\n",
      "Processing leaf group with 144 nodes...\n",
      "Processing leaf group with 15 nodes...\n",
      "Processing leaf group with 125 nodes...\n",
      "Processing leaf group with 8 nodes...\n"
     ]
    }
   ],
   "source": [
    "# Tạo cạnh chỉ giữa các node trong cùng nhóm lá\n",
    "for leaf_tuple, node_indices in leaf_groups.items():\n",
    "    if len(node_indices) < 2:\n",
    "        continue\n",
    "    print(f\"Processing leaf group with {len(node_indices)} nodes...\")\n",
    "    node_indices = np.array(node_indices)\n",
    "    \n",
    "    # Xử lý theo batch\n",
    "    for start in range(0, len(node_indices), batch_size):\n",
    "        end = min(start + batch_size, len(node_indices))\n",
    "        batch_nodes = node_indices[start:end]\n",
    "        \n",
    "        batch_edges = []\n",
    "        batch_weights = []\n",
    "        for idx_i in range(len(batch_nodes)):\n",
    "            i = batch_nodes[idx_i]\n",
    "            for idx_j in range(idx_i + 1, len(batch_nodes)):\n",
    "                j = batch_nodes[idx_j]\n",
    "                same_leaf_count = np.sum(leaf_indices[i] == leaf_indices[j])\n",
    "                freq = same_leaf_count / 3\n",
    "                price_similarity = abs(amounts[i] - amounts[j]) / (amounts[i] + amounts[j] + 1e-5)\n",
    "                if price_similarity < 0.5 and freq > 0.7:  # Tăng ngưỡng để giảm số lượng cạnh\n",
    "                    weight = freq * (1.0 - price_similarity)\n",
    "                    batch_edges.append([i, j])\n",
    "                    batch_edges.append([j, i])\n",
    "                    batch_weights.append(weight)\n",
    "                    batch_weights.append(weight)\n",
    "        \n",
    "        # Ghi batch_edges và batch_weights vào file\n",
    "        if batch_edges:\n",
    "            batch_edges_array = np.array(batch_edges, dtype=np.int64)\n",
    "            batch_weights_array = np.array(batch_weights, dtype=np.float32)\n",
    "            with open(edges_file, 'ab') as f:\n",
    "                np.save(f, batch_edges_array)\n",
    "            with open(weights_file, 'ab') as f:\n",
    "                np.save(f, batch_weights_array)\n",
    "            total_edges += len(batch_edges)\n",
    "        \n",
    "        # Giải phóng bộ nhớ\n",
    "        del batch_edges, batch_weights\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "579423b3-12e6-4d9d-8214-474c1646ee3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total SIMILAR_LEAF edges: 29841204\n"
     ]
    }
   ],
   "source": [
    "# Đọc lại edges và weights từ file\n",
    "print(f\"Total SIMILAR_LEAF edges: {total_edges}\")\n",
    "if total_edges > 0:\n",
    "    all_edges = np.empty((total_edges, 2), dtype=np.int64)\n",
    "    all_weights = np.empty(total_edges, dtype=np.float32)\n",
    "    \n",
    "    edges_pos = 0\n",
    "    with open(edges_file, 'rb') as f_edges, open(weights_file, 'rb') as f_weights:\n",
    "        while edges_pos < total_edges:\n",
    "            chunk_edges = np.load(f_edges)\n",
    "            chunk_weights = np.load(f_weights)\n",
    "            chunk_size = len(chunk_edges)\n",
    "            all_edges[edges_pos:edges_pos + chunk_size] = chunk_edges\n",
    "            all_weights[edges_pos:edges_pos + chunk_size] = chunk_weights\n",
    "            edges_pos += chunk_size\n",
    "else:\n",
    "    all_edges = np.empty((0, 2), dtype=np.int64)\n",
    "    all_weights = np.empty(0, dtype=np.float32)\n",
    "\n",
    "# Xóa file tạm\n",
    "os.remove(edges_file)\n",
    "os.remove(weights_file)\n",
    "\n",
    "# Chuyển sang tensor\n",
    "edge_index = torch.tensor(all_edges, dtype=torch.long).t().contiguous()\n",
    "edge_weight = torch.tensor(all_weights, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0293a499-8e11-408f-8528-315ac8413d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph: 34000 nodes, 29841204 edges\n",
      "Saving graph to .npz file...\n"
     ]
    }
   ],
   "source": [
    "del all_edges, all_weights\n",
    "gc.collect()\n",
    "\n",
    "# Tạo dữ liệu cho PyTorch Geometric\n",
    "data = Data(\n",
    "    x=torch.tensor(features.values, dtype=torch.float),\n",
    "    edge_index=edge_index,\n",
    "    edge_attr=edge_weight,\n",
    "    y=torch.tensor(y, dtype=torch.float)\n",
    ")\n",
    "\n",
    "print(f\"Graph: {data.num_nodes} nodes, {data.num_edges} edges\")\n",
    "\n",
    "# Lưu đồ thị vào file .npz\n",
    "print(\"Saving graph to .npz file...\")\n",
    "np.savez(\n",
    "    'store/graph_data.npz',\n",
    "    x=data.x.cpu().numpy(),\n",
    "    edge_index=data.edge_index.cpu().numpy(),\n",
    "    edge_attr=data.edge_attr.cpu().numpy(),\n",
    "    y=data.y.cpu().numpy()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c107203-98f2-4db3-b0d5-ac38ed993e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sử dụng NeighborSampler để tạo DataLoader\n",
    "train_loader = NeighborSampler(\n",
    "    data.edge_index,\n",
    "    node_idx=None,\n",
    "    sizes=[15, 10, 5],\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "045249b3-5bcf-42a0-907b-e67b24dcf387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GNN Model with GATConv\n",
    "class GNNModel(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=32):\n",
    "        super(GNNModel, self).__init__()\n",
    "        self.conv1 = GATConv(input_dim, hidden_dim, heads=1, concat=True)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(hidden_dim)\n",
    "        self.conv2 = GATConv(hidden_dim, 16, heads=1, concat=True)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(16)\n",
    "        self.conv3 = GATConv(16, 8, heads=1, concat=False)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(8)\n",
    "        self.fc = torch.nn.Linear(8, 1)\n",
    "    \n",
    "    def forward(self, x, edge_index, edge_weight, batch_size):\n",
    "        x = F.relu(self.bn1(self.conv1(x, edge_index, edge_attr=edge_weight)))\n",
    "        x = F.dropout(x, p=0.3, training=self.training)\n",
    "        x = F.relu(self.bn2(self.conv2(x, edge_index, edge_attr=edge_weight)))\n",
    "        x = F.dropout(x, p=0.3, training=self.training)\n",
    "        x = F.relu(self.bn3(self.conv3(x, edge_index, edge_attr=edge_weight)))\n",
    "        x = self.fc(x)\n",
    "        return x[:batch_size].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6467ad2a-5149-4b15-8656-700eabc9d3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Huấn luyện trên GPU với DataLoader\n",
    "model = GNNModel(input_dim=features.shape[1]).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10, factor=0.5)\n",
    "\n",
    "# Chuyển features và labels sang CPU\n",
    "data.x = data.x.to('cpu')\n",
    "data.y = data.y.to('cpu')\n",
    "data.edge_index = data.edge_index.to('cpu')\n",
    "data.edge_attr = data.edge_attr.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7620d903-93c9-41e6-8a12-93291a022a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50, Loss: 0.1048\n",
      "Sample Predictions: [15.429655 18.172083 15.634981 15.150665 17.145903]\n",
      "Sample True Values: [15.424949 18.064005 15.384127 14.84513  17.034386]\n",
      "Epoch 100, Loss: 0.0809\n",
      "Sample Predictions: [16.129534 15.601461 15.3193   16.160769 16.90637 ]\n",
      "Sample True Values: [16.489658  15.4641695 15.068274  15.894952  16.677711 ]\n",
      "Epoch 150, Loss: 0.0728\n",
      "Sample Predictions: [16.99467  16.51823  16.217943 17.280272 16.183828]\n",
      "Sample True Values: [16.811243 16.34124  15.907374 16.77042  15.761421]\n",
      "Epoch 200, Loss: 0.0700\n",
      "Sample Predictions: [16.401901 15.586923 15.754505 16.484446 15.222102]\n",
      "Sample True Values: [16.38046  15.444752 16.257858 16.666218 14.946913]\n",
      "Epoch 250, Loss: 0.0670\n",
      "Sample Predictions: [15.6609745 16.421482  15.549066  14.523319  15.318489 ]\n",
      "Sample True Values: [15.775605 16.562782 15.955577 13.764218 15.575091]\n",
      "Epoch 300, Loss: 0.0674\n",
      "Sample Predictions: [16.991417 14.652505 14.871898 17.87951  17.825125]\n",
      "Sample True Values: [16.98939  14.64842  14.914124 17.588272 17.663528]\n",
      "Epoch 350, Loss: 0.0673\n",
      "Sample Predictions: [15.5459385 15.906357  15.909384  17.084435  16.213358 ]\n",
      "Sample True Values: [15.319588 15.830414 15.687313 16.785925 15.60727 ]\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(400):\n",
    "    total_loss = 0\n",
    "    for batch_size, n_id, adjs in train_loader:\n",
    "        adj = adjs[-1]  # Lấy tầng cuối cùng\n",
    "        edge_index = adj.edge_index.to(device)\n",
    "        \n",
    "        # Lấy edge_weight tương ứng với edge_index của batch\n",
    "        edge_weight = data.edge_attr[adj.e_id] if adj.e_id is not None and len(adj.e_id) > 0 else None\n",
    "        if edge_weight is not None:\n",
    "            edge_weight = edge_weight.to(device)\n",
    "        else:\n",
    "            edge_weight = torch.ones(edge_index.size(1), device=device)  # Mặc định trọng số 1 nếu không có\n",
    "\n",
    "        x_batch = data.x[n_id].to(device)\n",
    "        y_batch = data.y[n_id[:batch_size]].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        out = model(x_batch, edge_index, edge_weight, batch_size)\n",
    "        loss = F.mse_loss(out, y_batch)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * batch_size\n",
    "    \n",
    "    total_loss /= data.num_nodes\n",
    "    scheduler.step(total_loss)\n",
    "    \n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f'Epoch {epoch + 1}, Loss: {total_loss:.4f}')\n",
    "        with torch.no_grad():\n",
    "            sample_pred = out[:5].cpu().numpy()\n",
    "            sample_true = y_batch[:5].cpu().numpy()\n",
    "            print(f\"Sample Predictions: {sample_pred}\")\n",
    "            print(f\"Sample True Values: {sample_true}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058fb78f-5193-470b-8670-229dfb5728ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess test data\n",
    "numerical_cols_input = ['Carpet Area', 'Super Area', 'Bathroom', 'Balcony', 'Current Floor', \n",
    "                        'Total Floors', 'BHK', 'Price', 'Car Parking', 'Price_x_SuperArea']\n",
    "categorical_cols_input = ['Transaction', 'Furnishing', 'Overlooking', 'Ownership', 'Facing']\n",
    "target_col = 'Amount'\n",
    "\n",
    "# Đảm bảo các giá trị từ huấn luyện được sử dụng\n",
    "test_features, test_y, _, _, _, _ = preprocess_data(\n",
    "    test_df, numerical_cols_input, categorical_cols_input, target_col, \n",
    "    scaler=scaler, encoders=encoders, target_encoders=target_encoders, target_scaler=target_scaler, is_train=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e246b60-7992-4d42-913c-0677e2c401a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo đồ thị cho test (dùng SIMILAR_LEAF tương tự)\n",
    "test_leaf_indices = np.zeros((len(test_df), 3), dtype=np.int32)\n",
    "for tree_idx in range(3):\n",
    "    test_leaf_indices[:, tree_idx] = assign_to_leaf(test_df)\n",
    "\n",
    "test_leaf_groups = defaultdict(list)\n",
    "for idx in range(len(test_df)):\n",
    "    leaf_tuple = tuple(test_leaf_indices[idx])\n",
    "    test_leaf_groups[leaf_tuple].append(idx)\n",
    "\n",
    "test_edges_file = 'store/test_similar_leaf_edges.npy'\n",
    "test_weights_file = 'store/test_similar_leaf_weights.npy'\n",
    "if os.path.exists(test_edges_file):\n",
    "    os.remove(test_edges_file)\n",
    "if os.path.exists(test_weights_file):\n",
    "    os.remove(test_weights_file)\n",
    "\n",
    "test_total_edges = 0\n",
    "test_amounts = test_df['Amount'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a188e58-ebd8-4369-a42d-07e1728693e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for leaf_tuple, node_indices in test_leaf_groups.items():\n",
    "    if len(node_indices) < 2:\n",
    "        continue\n",
    "    print(f\"Processing test leaf group with {len(node_indices)} nodes...\")\n",
    "    node_indices = np.array(node_indices)\n",
    "    \n",
    "    for start in range(0, len(node_indices), batch_size):\n",
    "        end = min(start + batch_size, len(node_indices))\n",
    "        batch_nodes = node_indices[start:end]\n",
    "        \n",
    "        batch_edges = []\n",
    "        batch_weights = []\n",
    "        for idx_i in range(len(batch_nodes)):\n",
    "            i = batch_nodes[idx_i]\n",
    "            for idx_j in range(idx_i + 1, len(batch_nodes)):\n",
    "                j = batch_nodes[idx_j]\n",
    "                same_leaf_count = np.sum(test_leaf_indices[i] == test_leaf_indices[j])\n",
    "                freq = same_leaf_count / 3\n",
    "                price_similarity = abs(test_amounts[i] - test_amounts[j]) / (test_amounts[i] + test_amounts[j] + 1e-5)\n",
    "                if price_similarity < 0.5 and freq > 0.7:\n",
    "                    weight = freq * (1.0 - price_similarity)\n",
    "                    batch_edges.append([i, j])\n",
    "                    batch_edges.append([j, i])\n",
    "                    batch_weights.append(weight)\n",
    "                    batch_weights.append(weight)\n",
    "        \n",
    "        if batch_edges:\n",
    "            batch_edges_array = np.array(batch_edges, dtype=np.int64)\n",
    "            batch_weights_array = np.array(batch_weights, dtype=np.float32)\n",
    "            with open(test_edges_file, 'ab') as f:\n",
    "                np.save(f, batch_edges_array)\n",
    "            with open(test_weights_file, 'ab') as f:\n",
    "                np.save(f, batch_weights_array)\n",
    "            test_total_edges += len(batch_edges)\n",
    "        \n",
    "        del batch_edges, batch_weights\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed8b9ea-73ac-4360-be48-89c192b6e622",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total test SIMILAR_LEAF edges: {test_total_edges}\")\n",
    "if test_total_edges > 0:\n",
    "    test_all_edges = np.empty((test_total_edges, 2), dtype=np.int64)\n",
    "    test_all_weights = np.empty(test_total_edges, dtype=np.float32)\n",
    "    \n",
    "    edges_pos = 0\n",
    "    with open(test_edges_file, 'rb') as f_edges, open(test_weights_file, 'rb') as f_weights:\n",
    "        while edges_pos < test_total_edges:\n",
    "            chunk_edges = np.load(f_edges)\n",
    "            chunk_weights = np.load(f_weights)\n",
    "            chunk_size = len(chunk_edges)\n",
    "            test_all_edges[edges_pos:edges_pos + chunk_size] = chunk_edges\n",
    "            test_all_weights[edges_pos:edges_pos + chunk_size] = chunk_weights\n",
    "            edges_pos += chunk_size\n",
    "else:\n",
    "    test_all_edges = np.empty((0, 2), dtype=np.int64)\n",
    "    test_all_weights = np.empty(0, dtype=np.float32)\n",
    "\n",
    "os.remove(test_edges_file)\n",
    "os.remove(test_weights_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2047314f-b7bf-4e9d-ac0b-d908dba4cfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_edge_index = torch.tensor(test_all_edges, dtype=torch.long).t().contiguous()\n",
    "test_edge_weight = torch.tensor(test_all_weights, dtype=torch.float)\n",
    "\n",
    "del test_all_edges, test_all_weights\n",
    "gc.collect()\n",
    "\n",
    "test_data = Data(\n",
    "    x=torch.tensor(test_features.values, dtype=torch.float),\n",
    "    edge_index=test_edge_index,\n",
    "    edge_attr=test_edge_weight,\n",
    "    y=torch.tensor(test_y, dtype=torch.float)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9384cddb-a1bb-43c9-a95e-5ad42f1fbd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Test Graph: {test_data.num_nodes} nodes, {test_data.num_edges} edges\")\n",
    "\n",
    "# Lưu test data vào file .npz\n",
    "print(\"Saving test graph to .npz file...\")\n",
    "np.savez(\n",
    "    'store/test_graph_data.npz',\n",
    "    x=test_data.x.cpu().numpy(),\n",
    "    edge_index=test_data.edge_index.cpu().numpy(),\n",
    "    edge_attr=test_data.edge_attr.cpu().numpy(),\n",
    "    y=test_data.y.cpu().numpy()\n",
    ")\n",
    "\n",
    "# DataLoader cho test data\n",
    "test_loader = NeighborSampler(\n",
    "    test_data.edge_index,\n",
    "    node_idx=None,\n",
    "    sizes=[15, 10, 5],\n",
    "    batch_size=1024,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a69d45d-4f53-4031-bf49-efbfb8a290a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chuyển test data sang CPU\n",
    "test_data.x = test_data.x.to('cpu')\n",
    "test_data.y = test_data.y.to('cpu')\n",
    "test_data.edge_index = test_data.edge_index.to('cpu')\n",
    "test_data.edge_attr = test_data.edge_attr.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e2e8e4-99fe-4c7e-bd47-7deacf6af1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Đánh giá mô hình\n",
    "model.eval()\n",
    "preds = []\n",
    "trues = []\n",
    "with torch.no_grad():\n",
    "    for batch_size, n_id, adjs in test_loader:\n",
    "        adj = adjs[-1]\n",
    "        edge_index = adj.edge_index.to(device)\n",
    "        \n",
    "        # Lấy edge_weight tương ứng với edge_index của batch\n",
    "        edge_weight = test_data.edge_attr[adj.e_id] if adj.e_id is not None and len(adj.e_id) > 0 else None\n",
    "        if edge_weight is not None:\n",
    "            edge_weight = edge_weight.to(device)\n",
    "        else:\n",
    "            edge_weight = torch.ones(edge_index.size(1), device=device)  # Mặc định trọng số 1 nếu không có\n",
    "\n",
    "        x_batch = test_data.x[n_id].to(device)\n",
    "        y_batch = test_data.y[n_id[:batch_size]].to(device)\n",
    "        \n",
    "        out = model(x_batch, edge_index, edge_weight, batch_size)\n",
    "        preds.append(np.expm1(out.cpu().numpy()))\n",
    "        trues.append(np.expm1(y_batch.cpu().numpy()))\n",
    "\n",
    "pred = np.concatenate(preds)\n",
    "true = np.concatenate(trues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6daf012a-9338-4677-8c3f-f6c383e61850",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = mean_absolute_error(true, pred)\n",
    "mse = mean_squared_error(true, pred)\n",
    "r2 = r2_score(true, pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mape = np.mean(np.abs((true - pred) / true)) * 100\n",
    "\n",
    "print(\"\\nGNN Test Metrics:\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"R2 Score: {r2:.4f}\")\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "print(f\"MAPE: {mape:.2f}%\")\n",
    "\n",
    "# Lưu mô hình\n",
    "torch.save(model.state_dict(), 'store/gnn_model_with_random_forest.pth')\n",
    "\n",
    "# Clear memory\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda-gpt",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
