{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3d44836-fbad-4eb6-a8b1-6bb31bf01ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from neo4j import GraphDatabase\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97ac1eda-3f2c-4ef4-9104-42538236ae18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Clear memory\n",
    "gc.collect()\n",
    "\n",
    "# Kết nối tới Neo4j\n",
    "class Neo4jConnection:\n",
    "    def __init__(self, uri, user, password):\n",
    "        self.driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "    \n",
    "    def close(self):\n",
    "        self.driver.close()\n",
    "    \n",
    "    def run_query(self, query, parameters=None):\n",
    "        with self.driver.session() as session:\n",
    "            session.run(query, parameters)\n",
    "    \n",
    "    def get_nodes(self):\n",
    "        with self.driver.session() as session:\n",
    "            result = session.run(\"\"\"\n",
    "                MATCH (h:House)\n",
    "                RETURN h.house_id AS house_id,\n",
    "                       h.carpet_area AS carpet_area,\n",
    "                       h.super_area AS super_area,\n",
    "                       h.bathroom AS bathroom,\n",
    "                       h.balcony AS balcony,\n",
    "                       h.current_floor AS current_floor,\n",
    "                       h.total_floors AS total_floors,\n",
    "                       h.bhk AS bhk,\n",
    "                       h.price AS price,\n",
    "                       h.car_parking AS car_parking,\n",
    "                       h.price_x_super_area AS price_x_super_area,\n",
    "                       h.amount AS amount,\n",
    "                       h.transaction AS transaction,\n",
    "                       h.furnishing AS furnishing,\n",
    "                       h.overlooking AS overlooking,\n",
    "                       h.ownership AS ownership,\n",
    "                       h.facing AS facing\n",
    "            \"\"\")\n",
    "            return pd.DataFrame([record.values() for record in result], columns=result.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2aad8d1-89bc-4271-acf0-da470c01f88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Đọc dữ liệu\n",
    "train_df = pd.read_csv('data2/train.csv')\n",
    "test_df = pd.read_csv('data2/test.csv')\n",
    "\n",
    "# Feature engineering: Add Price * Super Area\n",
    "train_df['Price_x_SuperArea'] = train_df['Price'] * train_df['Super Area']\n",
    "test_df['Price_x_SuperArea'] = test_df['Price'] * test_df['Super Area']\n",
    "\n",
    "# Thêm cột index làm house_id\n",
    "train_df['index'] = train_df.index\n",
    "test_df['index'] = test_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "171473b4-607a-41a1-8c06-fd2ba65310f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm để gán node vào lá của một cây (độ sâu 3)\n",
    "def assign_to_leaf(df):\n",
    "    leaf_ids = np.zeros(len(df), dtype=np.int32)\n",
    "    for idx in range(len(df)):\n",
    "        price = df.iloc[idx]['Price']\n",
    "        super_area = df.iloc[idx]['Super Area']\n",
    "        carpet_area = df.iloc[idx]['Carpet Area']\n",
    "        total_floors = df.iloc[idx]['Total Floors']\n",
    "        \n",
    "        # Cây giống nhau, áp dụng quy tắc phân chia đến độ sâu 3\n",
    "        if price <= 10308.50:\n",
    "            if super_area <= 1800.04:\n",
    "                if carpet_area <= 1915.50:\n",
    "                    if price <= 5056.50:\n",
    "                        leaf_ids[idx] = 0  # Nhánh 1.1.1\n",
    "                    else:\n",
    "                        leaf_ids[idx] = 1  # Nhánh 1.1.2\n",
    "                else:\n",
    "                    if total_floors <= 4.50:\n",
    "                        leaf_ids[idx] = 2  # Nhánh 1.2.1\n",
    "                    else:\n",
    "                        leaf_ids[idx] = 3  # Nhánh 1.2.2\n",
    "            else:\n",
    "                if price <= 6952.00:\n",
    "                    if super_area <= 3375.00:\n",
    "                        leaf_ids[idx] = 4  # Nhánh 1.3.1\n",
    "                    else:\n",
    "                        leaf_ids[idx] = 5  # Nhánh 1.3.2\n",
    "                else:\n",
    "                    if super_area <= 3331.01:\n",
    "                        leaf_ids[idx] = 6  # Nhánh 1.4.1\n",
    "                    else:\n",
    "                        leaf_ids[idx] = 7  # Nhánh 1.4.2\n",
    "        else:\n",
    "            if super_area <= 2592.52:\n",
    "                if price <= 18905.50:\n",
    "                    if super_area <= 1450.00:\n",
    "                        leaf_ids[idx] = 8  # Nhánh 2.1.1\n",
    "                    else:\n",
    "                        leaf_ids[idx] = 9  # Nhánh 2.1.2\n",
    "                else:\n",
    "                    if super_area <= 1150.00:\n",
    "                        leaf_ids[idx] = 10  # Nhánh 2.2.1\n",
    "                    else:\n",
    "                        leaf_ids[idx] = 11  # Nhánh 2.2.2\n",
    "            else:\n",
    "                if price <= 29375.00:\n",
    "                    if price <= 15997.50:\n",
    "                        leaf_ids[idx] = 12  # Nhánh 2.3.1\n",
    "                    else:\n",
    "                        leaf_ids[idx] = 13  # Nhánh 2.3.2\n",
    "                else:\n",
    "                    if super_area <= 3924.96:\n",
    "                        leaf_ids[idx] = 14  # Nhánh 2.4.1\n",
    "                    else:\n",
    "                        leaf_ids[idx] = 15  # Nhánh 2.4.2\n",
    "    return leaf_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "751b0aa4-8b66-4079-8bf2-d11d602efdbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating SIMILAR_LEAF edges from Random Forest structure...\n"
     ]
    }
   ],
   "source": [
    "# Tạo SIMILAR_LEAF edges từ cấu trúc cây\n",
    "print(\"Creating SIMILAR_LEAF edges from Random Forest structure...\")\n",
    "# Mỗi cây có 16 lá, có 3 cây giống nhau\n",
    "leaf_indices = np.zeros((len(train_df), 3), dtype=np.int32)\n",
    "for tree_idx in range(3):\n",
    "    leaf_indices[:, tree_idx] = assign_to_leaf(train_df)\n",
    "\n",
    "# Tính tần suất hai node rơi vào cùng lá\n",
    "similar_leaf_edges = []\n",
    "similar_leaf_weights = []\n",
    "amounts = train_df['Amount'].values\n",
    "for i in range(len(train_df)):\n",
    "    for j in range(i + 1, len(train_df)):\n",
    "        # Đếm số cây mà i và j rơi vào cùng lá\n",
    "        same_leaf_count = np.sum(leaf_indices[i] == leaf_indices[j])\n",
    "        if same_leaf_count > 0:  # Chỉ tạo cạnh nếu có ít nhất 1 cây\n",
    "            # Tính trọng số dựa trên tần suất và độ tương đồng giá\n",
    "            freq = same_leaf_count / 3  # 3 cây\n",
    "            price_similarity = abs(amounts[i] - amounts[j]) / (amounts[i] + amounts[j] + 1e-5)\n",
    "            if price_similarity < 0.2 and freq > 0.3:  # Ngưỡng để giảm số lượng cạnh\n",
    "                weight = freq * (1.0 - price_similarity)\n",
    "                similar_leaf_edges.append([i, j])\n",
    "                similar_leaf_edges.append([j, i])\n",
    "                similar_leaf_weights.append(weight)\n",
    "                similar_leaf_weights.append(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7937d29-2f87-4634-998e-431dc3adcecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kết nối tới Neo4j\n",
    "neo4j_conn = Neo4jConnection(\"bolt://localhost:7687\", \"neo4j\", \"hqiineo4j\")  # Thay bằng thông tin của bạn\n",
    "\n",
    "# Xóa dữ liệu cũ (nếu cần)\n",
    "neo4j_conn.run_query(\"MATCH (n) DETACH DELETE n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a9a4d0a-b7c3-480d-8f08-3559de996b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating House nodes...\n"
     ]
    }
   ],
   "source": [
    "# Tạo node House\n",
    "print(\"Creating House nodes...\")\n",
    "for _, row in train_df.iterrows():\n",
    "    query = \"\"\"\n",
    "        CREATE (h:House {\n",
    "            house_id: $house_id,\n",
    "            carpet_area: $carpet_area,\n",
    "            super_area: $super_area,\n",
    "            bathroom: $bathroom,\n",
    "            balcony: $balcony,\n",
    "            current_floor: $current_floor,\n",
    "            total_floors: $total_floors,\n",
    "            bhk: $bhk,\n",
    "            price: $price,\n",
    "            car_parking: $car_parking,\n",
    "            price_x_super_area: $price_x_super_area,\n",
    "            amount: $amount,\n",
    "            transaction: $transaction,\n",
    "            furnishing: $furnishing,\n",
    "            overlooking: $overlooking,\n",
    "            society: $society,\n",
    "            ownership: $ownership,\n",
    "            facing: $facing,\n",
    "            location: $location\n",
    "        })\n",
    "    \"\"\"\n",
    "    parameters = {\n",
    "        \"house_id\": int(row['index']),\n",
    "        \"carpet_area\": float(row['Carpet Area']),\n",
    "        \"super_area\": float(row['Super Area']),\n",
    "        \"bathroom\": int(row['Bathroom']),\n",
    "        \"balcony\": int(row['Balcony']),\n",
    "        \"current_floor\": int(row['Current Floor']),\n",
    "        \"total_floors\": int(row['Total Floors']),\n",
    "        \"bhk\": int(row['BHK']),\n",
    "        \"price\": float(row['Price']),\n",
    "        \"car_parking\": int(row['Car Parking']),\n",
    "        \"price_x_super_area\": float(row['Price_x_SuperArea']),\n",
    "        \"amount\": float(row['Amount']),\n",
    "        \"transaction\": row['Transaction'],\n",
    "        \"furnishing\": row['Furnishing'],\n",
    "        \"overlooking\": row['Overlooking'],\n",
    "        \"society\": row['Society'],\n",
    "        \"ownership\": row['Ownership'],\n",
    "        \"facing\": row['Facing'],\n",
    "        \"location\": row['Location']\n",
    "    }\n",
    "    neo4j_conn.run_query(query, parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90529778-629d-413c-a503-a4cae9df836a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Society nodes...\n"
     ]
    }
   ],
   "source": [
    "# Tạo node Society\n",
    "print(\"Creating Society nodes...\")\n",
    "societies = train_df['Society'].unique()\n",
    "for society in societies:\n",
    "    query = \"\"\"\n",
    "        MERGE (s:Society {name: $name})\n",
    "    \"\"\"\n",
    "    neo4j_conn.run_query(query, {\"name\": society})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af16683f-255c-47ee-93e1-dbcdd4524a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Location nodes...\n"
     ]
    }
   ],
   "source": [
    "# Tạo node Location\n",
    "print(\"Creating Location nodes...\")\n",
    "locations = train_df['Location'].unique()\n",
    "for location in locations:\n",
    "    query = \"\"\"\n",
    "        MERGE (l:Location {name: $name})\n",
    "    \"\"\"\n",
    "    neo4j_conn.run_query(query, {\"name\": location})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2aeab055-5447-4560-b159-608ff7e3bbc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating BELONGS_TO_SOCIETY relationships...\n"
     ]
    }
   ],
   "source": [
    "# Tạo relationship BELONGS_TO_SOCIETY\n",
    "print(\"Creating BELONGS_TO_SOCIETY relationships...\")\n",
    "for _, row in train_df.iterrows():\n",
    "    query = \"\"\"\n",
    "        MATCH (h:House {house_id: $house_id})\n",
    "        MATCH (s:Society {name: $society})\n",
    "        CREATE (h)-[:BELONGS_TO_SOCIETY]->(s)\n",
    "    \"\"\"\n",
    "    parameters = {\n",
    "        \"house_id\": int(row['index']),\n",
    "        \"society\": row['Society']\n",
    "    }\n",
    "    neo4j_conn.run_query(query, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "893d720a-e462-4bcd-ae5e-c45cd2d7564b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating BELONGS_TO_LOCATION relationships...\n"
     ]
    }
   ],
   "source": [
    "# Tạo relationship BELONGS_TO_LOCATION\n",
    "print(\"Creating BELONGS_TO_LOCATION relationships...\")\n",
    "for _, row in train_df.iterrows():\n",
    "    query = \"\"\"\n",
    "        MATCH (h:House {house_id: $house_id})\n",
    "        MATCH (l:Location {name: $location})\n",
    "        CREATE (h)-[:BELONGS_TO_LOCATION]->(l)\n",
    "    \"\"\"\n",
    "    parameters = {\n",
    "        \"house_id\": int(row['index']),\n",
    "        \"location\": row['Location']\n",
    "    }\n",
    "    neo4j_conn.run_query(query, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30114862-c3d0-45e0-8e81-9ee27f5f5cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data from Neo4j...\n"
     ]
    }
   ],
   "source": [
    "# Trích xuất dữ liệu từ Neo4j\n",
    "print(\"Extracting data from Neo4j...\")\n",
    "nodes_df = neo4j_conn.get_nodes()\n",
    "\n",
    "# Đóng kết nối Neo4j\n",
    "neo4j_conn.close()\n",
    "\n",
    "# Tạo đặc trưng cho GNN\n",
    "numerical_cols = ['carpet_area', 'super_area', 'bathroom', 'balcony', 'current_floor', \n",
    "                 'total_floors', 'bhk', 'price', 'car_parking', 'price_x_super_area']\n",
    "categorical_cols = ['transaction', 'furnishing', 'overlooking', 'ownership', 'facing']\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "numerical_features = scaler.fit_transform(nodes_df[numerical_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa5bb074-5302-418e-85d0-c206058b4568",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_features = []\n",
    "encoders = {}\n",
    "for col in categorical_cols:\n",
    "    encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "    encoded = encoder.fit_transform(nodes_df[[col]])\n",
    "    encoded_df = pd.DataFrame(encoded, columns=[f\"{col}_{cat}\" for cat in encoder.categories_[0]])\n",
    "    encoded_features.append(encoded_df)\n",
    "    encoders[col] = encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b30c8951-b060-4029-8e68-ea7c8124a5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.concat([pd.DataFrame(numerical_features, columns=numerical_cols)] + encoded_features, axis=1)\n",
    "y = np.log1p(nodes_df['amount'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3d9c404-d36b-4a7e-82b4-301db61f30ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating SIMILAR_BHK edges using KNN...\n",
      "Processing BHK=3...\n",
      "Processing BHK=4...\n",
      "Processing BHK=2...\n",
      "Processing BHK=1...\n",
      "Processing BHK=6...\n",
      "Processing BHK=5...\n",
      "Processing BHK=8...\n",
      "Processing BHK=10...\n",
      "Processing BHK=7...\n",
      "Processing BHK=9...\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating SIMILAR_BHK edges using KNN...\")\n",
    "bhk_values = train_df['BHK'].unique()\n",
    "similar_bhk_edges = []\n",
    "similar_bhk_weights = []\n",
    "for bhk in bhk_values:\n",
    "    print(f\"Processing BHK={bhk}...\")\n",
    "    mask = train_df['BHK'] == bhk\n",
    "    indices = train_df[mask].index\n",
    "    if len(indices) < 2:\n",
    "        continue\n",
    "    bhk_features = features.loc[indices, numerical_cols].values\n",
    "    bhk_amounts = train_df.loc[indices, 'Amount'].values\n",
    "    knn = NearestNeighbors(n_neighbors=min(10, len(indices)), metric='cosine', n_jobs=-1)\n",
    "    knn.fit(bhk_features)\n",
    "    distances, neighbors = knn.kneighbors(bhk_features)\n",
    "    for i in range(len(indices)):\n",
    "        for j_idx, dist in zip(neighbors[i][1:], distances[i][1:]):  # Bỏ qua chính nó\n",
    "            j = j_idx  # Sửa ở đây: j_idx đã là chỉ số trong bhk_features\n",
    "            idx_i, idx_j = indices[i], indices[j]\n",
    "            sim = 1 - dist\n",
    "            price_similarity = abs(bhk_amounts[i] - bhk_amounts[j]) / (bhk_amounts[i] + bhk_amounts[j] + 1e-5)\n",
    "            if price_similarity < 0.2:\n",
    "                similar_bhk_edges.append([idx_i, idx_j])\n",
    "                similar_bhk_edges.append([idx_j, idx_i])\n",
    "                weight = 1.0 - price_similarity\n",
    "                similar_bhk_weights.append(weight)\n",
    "                similar_bhk_weights.append(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "daff2adf-73f0-46fb-b7d4-ac41b2cea721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating SIMILAR_PRICE edges using KNN...\n",
      "Processing Amount range: 99999.999 to 3000000.0...\n",
      "Processing Amount range: 3000000.0 to 4200000.0...\n",
      "Processing Amount range: 4200000.0 to 5450000.0...\n",
      "Processing Amount range: 5450000.0 to 6500000.0...\n",
      "Processing Amount range: 6500000.0 to 8000000.0...\n",
      "Processing Amount range: 8000000.0 to 9500000.0...\n",
      "Processing Amount range: 9500000.0 to 13000000.0...\n",
      "Processing Amount range: 13000000.0 to 17500000.0...\n",
      "Processing Amount range: 17500000.0 to 25000000.0...\n",
      "Processing Amount range: 25000000.0 to 368000000.0...\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating SIMILAR_PRICE edges using KNN...\")\n",
    "amount_bins = pd.qcut(train_df['Amount'], q=10, duplicates='drop').cat.categories\n",
    "similar_price_edges = []\n",
    "similar_price_weights = []\n",
    "for i in range(len(amount_bins)):\n",
    "    min_amount = amount_bins[i].left\n",
    "    max_amount = amount_bins[i].right\n",
    "    print(f\"Processing Amount range: {min_amount} to {max_amount}...\")\n",
    "    mask = (train_df['Amount'] >= min_amount) & (train_df['Amount'] <= max_amount)\n",
    "    indices = train_df[mask].index\n",
    "    if len(indices) < 2:\n",
    "        continue\n",
    "    price_features = features.loc[indices, numerical_cols].values\n",
    "    price_amounts = train_df.loc[indices, 'Amount'].values\n",
    "    knn = NearestNeighbors(n_neighbors=min(10, len(indices)), metric='cosine', n_jobs=-1)\n",
    "    knn.fit(price_features)\n",
    "    distances, neighbors = knn.kneighbors(price_features)\n",
    "    for i in range(len(indices)):\n",
    "        for j_idx, dist in zip(neighbors[i][1:], distances[i][1:]):\n",
    "            j = j_idx  # Sửa ở đây: j_idx đã là chỉ số trong price_features\n",
    "            idx_i, idx_j = indices[i], indices[j]\n",
    "            sim = 1 - dist\n",
    "            price_similarity = abs(price_amounts[i] - price_amounts[j]) / (price_amounts[i] + price_amounts[j] + 1e-5)\n",
    "            if price_similarity < 0.15:\n",
    "                similar_price_edges.append([idx_i, idx_j])\n",
    "                similar_price_edges.append([idx_j, idx_i])\n",
    "                weight = 1.0 - price_similarity\n",
    "                similar_price_weights.append(weight)\n",
    "                similar_price_weights.append(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7cb9235-09fe-46f9-b616-6b4a281ab022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kết hợp tất cả các cạnh\n",
    "edges = similar_leaf_edges + similar_bhk_edges + similar_price_edges\n",
    "weights = similar_leaf_weights + similar_bhk_weights + similar_price_weights\n",
    "edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "edge_weight = torch.tensor(weights, dtype=torch.float)\n",
    "\n",
    "# Tạo dữ liệu cho PyTorch Geometric\n",
    "data = Data(\n",
    "    x=torch.tensor(features.values, dtype=torch.float),\n",
    "    edge_index=edge_index,\n",
    "    edge_attr=edge_weight,\n",
    "    y=torch.tensor(y, dtype=torch.float)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8bc45eb6-1bce-494a-91c2-b3504afd041b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph: 34000 nodes, 152461422 edges\n"
     ]
    }
   ],
   "source": [
    "# Kiểm tra số lượng đỉnh và cạnh\n",
    "print(f\"Graph: {data.num_nodes} nodes, {data.num_edges} edges\")\n",
    "\n",
    "# GNN Model with GATConv\n",
    "class GNNModel(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=64):\n",
    "        super(GNNModel, self).__init__()\n",
    "        self.conv1 = GATConv(input_dim, hidden_dim, heads=2, concat=True)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(hidden_dim * 2)\n",
    "        self.conv2 = GATConv(hidden_dim * 2, 32, heads=2, concat=True)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(32 * 2)\n",
    "        self.conv3 = GATConv(32 * 2, 16, heads=1, concat=False)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(16)\n",
    "        self.fc = torch.nn.Linear(16, 1)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_weight = data.x, data.edge_index, data.edge_attr\n",
    "        x = self.conv1(x, edge_index, edge_attr=edge_weight)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.3, training=self.training)\n",
    "        x = self.conv2(x, edge_index, edge_attr=edge_weight)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x, edge_index, edge_attr=edge_weight)\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc(x)\n",
    "        return x.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "04343f5e-38e1-4ce2-8a52-3ff0487b014d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training on CPU\n",
    "device = torch.device('cuda')\n",
    "model = GNNModel(input_dim=features.shape[1]).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10, factor=0.5)\n",
    "\n",
    "data = data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e92687a5-139f-4dde-841f-c64c237d7157",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.14 GiB. GPU 0 has a total capacity of 2.00 GiB of which 0 bytes is free. Of the allocated memory 7.01 GiB is allocated by PyTorch, and 1005.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1500\u001b[39m):\n\u001b[0;32m      3\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m----> 4\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     mse_loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(out, data\u001b[38;5;241m.\u001b[39my)\n\u001b[0;32m      6\u001b[0m     mae_loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39ml1_loss(out, data\u001b[38;5;241m.\u001b[39my)\n",
      "File \u001b[1;32mG:\\MyProject\\cuda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mG:\\MyProject\\cuda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[18], line 18\u001b[0m, in \u001b[0;36mGNNModel.forward\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, data):\n\u001b[0;32m     17\u001b[0m     x, edge_index, edge_weight \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mx, data\u001b[38;5;241m.\u001b[39medge_index, data\u001b[38;5;241m.\u001b[39medge_attr\n\u001b[1;32m---> 18\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(x)\n\u001b[0;32m     20\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(x)\n",
      "File \u001b[1;32mG:\\MyProject\\cuda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mG:\\MyProject\\cuda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mG:\\MyProject\\cuda\\Lib\\site-packages\\torch_geometric\\nn\\conv\\gat_conv.py:347\u001b[0m, in \u001b[0;36mGATConv.forward\u001b[1;34m(self, x, edge_index, edge_attr, size, return_attention_weights)\u001b[0m\n\u001b[0;32m    345\u001b[0m         num_nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(num_nodes, x_dst\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m    346\u001b[0m     num_nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(size) \u001b[38;5;28;01mif\u001b[39;00m size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m num_nodes\n\u001b[1;32m--> 347\u001b[0m     edge_index, edge_attr \u001b[38;5;241m=\u001b[39m \u001b[43mremove_self_loops\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m        \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    349\u001b[0m     edge_index, edge_attr \u001b[38;5;241m=\u001b[39m add_self_loops(\n\u001b[0;32m    350\u001b[0m         edge_index, edge_attr, fill_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfill_value,\n\u001b[0;32m    351\u001b[0m         num_nodes\u001b[38;5;241m=\u001b[39mnum_nodes)\n\u001b[0;32m    352\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(edge_index, SparseTensor):\n",
      "File \u001b[1;32mG:\\MyProject\\cuda\\Lib\\site-packages\\torch_geometric\\utils\\loop.py:113\u001b[0m, in \u001b[0;36mremove_self_loops\u001b[1;34m(edge_index, edge_attr)\u001b[0m\n\u001b[0;32m    110\u001b[0m     is_undirected \u001b[38;5;241m=\u001b[39m edge_index\u001b[38;5;241m.\u001b[39mis_undirected\n\u001b[0;32m    112\u001b[0m mask \u001b[38;5;241m=\u001b[39m edge_index[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m edge_index[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m--> 113\u001b[0m edge_index \u001b[38;5;241m=\u001b[39m \u001b[43medge_index\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(edge_index, EdgeIndex):\n\u001b[0;32m    116\u001b[0m     edge_index\u001b[38;5;241m.\u001b[39m_is_undirected \u001b[38;5;241m=\u001b[39m is_undirected\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.14 GiB. GPU 0 has a total capacity of 2.00 GiB of which 0 bytes is free. Of the allocated memory 7.01 GiB is allocated by PyTorch, and 1005.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(1500):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    mse_loss = F.mse_loss(out, data.y)\n",
    "    mae_loss = F.l1_loss(out, data.y)\n",
    "    mse_weight = min(0.8, 0.2 + epoch / 2500)\n",
    "    mae_weight = 1 - mse_weight\n",
    "    loss = mse_weight * mse_loss + mae_weight * mae_loss\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step(loss)\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f'Epoch {epoch + 1}, Loss: {loss.item():.4f}, MSE Weight: {mse_weight:.2f}')\n",
    "        with torch.no_grad():\n",
    "            sample_pred = out[:5].cpu().numpy()\n",
    "            sample_true = data.y[:5].cpu().numpy()\n",
    "            print(f\"Sample Predictions: {sample_pred}\")\n",
    "            print(f\"Sample True Values: {sample_true}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861de4f1-1672-4760-b438-5b1df30962a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation (dùng tập test từ file)\n",
    "def preprocess_data(df, numerical_cols, categorical_cols, target_col, \n",
    "                   scaler=None, encoders=None, target_encoders=None, target_scaler=None, is_train=True):\n",
    "    df = df.copy()\n",
    "    \n",
    "    target_encode_cols = ['Society', 'Location', 'Overlooking']\n",
    "    \n",
    "    if is_train:\n",
    "        scaler = MinMaxScaler()\n",
    "        df[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n",
    "    else:\n",
    "        df[numerical_cols] = scaler.transform(df[numerical_cols])\n",
    "    \n",
    "    encoded_features = []\n",
    "    if is_train:\n",
    "        encoders = {}\n",
    "        target_encoders = {}\n",
    "        target_scaler = MinMaxScaler()\n",
    "        \n",
    "        one_hot_cols = ['Transaction', 'Furnishing', 'Ownership', 'Facing']\n",
    "        for col in one_hot_cols:\n",
    "            encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "            encoded = encoder.fit_transform(df[[col]])\n",
    "            encoded_df = pd.DataFrame(\n",
    "                encoded, \n",
    "                columns=[f\"{col}_{cat}\" for cat in encoder.categories_[0]]\n",
    "            )\n",
    "            encoded_features.append(encoded_df)\n",
    "            encoders[col] = encoder\n",
    "        \n",
    "        target_encoded = []\n",
    "        for col in target_encode_cols:\n",
    "            mean_target = df.groupby(col)[target_col].mean()\n",
    "            df[f'{col}_encoded'] = df[col].map(mean_target)\n",
    "            target_encoders[col] = mean_target\n",
    "            target_encoded.append(df[[f'{col}_encoded']])\n",
    "        \n",
    "        target_encoded_df = pd.concat(target_encoded, axis=1)\n",
    "        scaled_target_encoded = target_scaler.fit_transform(target_encoded_df)\n",
    "        for i, col in enumerate(target_encode_cols):\n",
    "            df[f'{col}_encoded'] = scaled_target_encoded[:, i]\n",
    "            encoded_features.append(df[[f'{col}_encoded']])\n",
    "    else:\n",
    "        for col in ['Transaction', 'Furnishing', 'Ownership', 'Facing']:\n",
    "            encoded = encoders[col].transform(df[[col]])\n",
    "            encoded_df = pd.DataFrame(\n",
    "                encoded, \n",
    "                columns=[f\"{col}_{cat}\" for cat in encoders[col].categories_[0]]\n",
    "            )\n",
    "            encoded_features.append(encoded_df)\n",
    "        \n",
    "        target_encoded = []\n",
    "        for col in target_encode_cols:\n",
    "            default_value = df[target_col].mean() if target_col in df else 0\n",
    "            df[f'{col}_encoded'] = df[col].map(target_encoders.get(col, pd.Series())).fillna(default_value)\n",
    "            target_encoded.append(df[[f'{col}_encoded']])\n",
    "        \n",
    "        target_encoded_df = pd.concat(target_encoded, axis=1)\n",
    "        scaled_target_encoded = target_scaler.transform(target_encoded_df)\n",
    "        for i, col in enumerate(target_encode_cols):\n",
    "            df[f'{col}_encoded'] = scaled_target_encoded[:, i]\n",
    "            encoded_features.append(df[[f'{col}_encoded']])\n",
    "    \n",
    "    feature_df = pd.concat([df[numerical_cols]] + encoded_features, axis=1)\n",
    "    y = np.log1p(df[target_col].values) if target_col in df else None\n",
    "    \n",
    "    return feature_df, y, scaler, encoders, target_encoders, target_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3a1dd5-e7af-443f-86a7-5132553bbe1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess test data\n",
    "numerical_cols_input = ['Carpet Area', 'Super Area', 'Bathroom', 'Balcony', 'Current Floor', \n",
    "                        'Total Floors', 'BHK', 'Price', 'Car Parking', 'Price_x_SuperArea']\n",
    "categorical_cols_input = ['Transaction', 'Furnishing', 'Overlooking', 'Ownership', 'Facing']\n",
    "target_col = 'Amount'\n",
    "\n",
    "test_features, test_y, _, _, _, _ = preprocess_data(\n",
    "    test_df, numerical_cols_input, categorical_cols_input, target_col, \n",
    "    scaler=scaler, encoders=encoders, target_encoders=None, target_scaler=None, is_train=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5fed4091-fc47-4deb-81f2-ebe8e3a7f9b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m     edge_weight \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(edge_weight, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m edge_index, edge_weight\n\u001b[1;32m---> 28\u001b[0m test_edge_index, test_edge_weight \u001b[38;5;241m=\u001b[39m create_edges(test_df, \u001b[43mtest_features\u001b[49m, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m, sim_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.6\u001b[39m)\n\u001b[0;32m     29\u001b[0m test_data \u001b[38;5;241m=\u001b[39m Data(\n\u001b[0;32m     30\u001b[0m     x\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mtensor(test_features\u001b[38;5;241m.\u001b[39mvalues, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat),\n\u001b[0;32m     31\u001b[0m     edge_index\u001b[38;5;241m=\u001b[39mtest_edge_index,\n\u001b[0;32m     32\u001b[0m     edge_attr\u001b[38;5;241m=\u001b[39mtest_edge_weight,\n\u001b[0;32m     33\u001b[0m     y\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mtensor(test_y, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\n\u001b[0;32m     34\u001b[0m )\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_features' is not defined"
     ]
    }
   ],
   "source": [
    "# Tạo đồ thị cho test\n",
    "def create_edges(df, feature_df, k=15, sim_threshold=0.6, key_cols=['Society', 'Location', 'BHK']):\n",
    "    from sklearn.neighbors import NearestNeighbors\n",
    "    features = feature_df.values\n",
    "    knn = NearestNeighbors(n_neighbors=min(k + 1, len(df)), metric='cosine', n_jobs=-1)\n",
    "    knn.fit(features)\n",
    "    distances, indices = knn.kneighbors(features)\n",
    "    \n",
    "    edge_index = []\n",
    "    edge_weight = []\n",
    "    \n",
    "    num_nodes = len(df)\n",
    "    for i in range(len(df)):\n",
    "        for j_idx, dist in zip(indices[i][1:], distances[i][1:]):\n",
    "            if j_idx >= num_nodes:\n",
    "                continue\n",
    "            sim = 1 - dist\n",
    "            if sim > sim_threshold and any(df.iloc[i][col] == df.iloc[j_idx][col] for col in key_cols):\n",
    "                edge_index.append([i, j_idx])\n",
    "                edge_index.append([j_idx, i])\n",
    "                edge_weight.append(sim)\n",
    "                edge_weight.append(sim)\n",
    "    \n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "    edge_weight = torch.tensor(edge_weight, dtype=torch.float)\n",
    "    return edge_index, edge_weight\n",
    "\n",
    "test_edge_index, test_edge_weight = create_edges(test_df, test_features, k=15, sim_threshold=0.6)\n",
    "test_data = Data(\n",
    "    x=torch.tensor(test_features.values, dtype=torch.float),\n",
    "    edge_index=test_edge_index,\n",
    "    edge_attr=test_edge_weight,\n",
    "    y=torch.tensor(test_y, dtype=torch.float)\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191e76ec-39e4-4cda-8196-c147485dee4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Đánh giá mô hình\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pred = model(test_data)\n",
    "    pred = np.expm1(pred.cpu().numpy())\n",
    "    true = np.expm1(test_data.y.cpu().numpy())\n",
    "    \n",
    "    mae = mean_absolute_error(true, pred)\n",
    "    mse = mean_squared_error(true, pred)\n",
    "    r2 = r2_score(true, pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mape = np.mean(np.abs((true - pred) / true)) * 100\n",
    "    \n",
    "    print(\"\\nGNN Test Metrics:\")\n",
    "    print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "    print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "    print(f\"R2 Score: {r2:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.2f}\")\n",
    "    print(f\"MAPE: {mape:.2f}%\")\n",
    "\n",
    "# Save model\n",
    "torch.save(model.state_dict(), 'gnn_model_with_random_forest_structure.pth')\n",
    "\n",
    "# Clear memory\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda-gpt",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
