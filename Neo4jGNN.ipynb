{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c3d44836-fbad-4eb6-a8b1-6bb31bf01ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from neo4j import GraphDatabase\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "97ac1eda-3f2c-4ef4-9104-42538236ae18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Clear memory\n",
    "gc.collect()\n",
    "\n",
    "# Kết nối tới Neo4j\n",
    "class Neo4jConnection:\n",
    "    def __init__(self, uri, user, password):\n",
    "        self.driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "    \n",
    "    def close(self):\n",
    "        self.driver.close()\n",
    "    \n",
    "    def run_query(self, query, parameters=None):\n",
    "        with self.driver.session() as session:\n",
    "            session.run(query, parameters)\n",
    "    \n",
    "    def get_nodes(self):\n",
    "        with self.driver.session() as session:\n",
    "            result = session.run(\"\"\"\n",
    "                MATCH (h:House)\n",
    "                RETURN h.house_id AS house_id,\n",
    "                       h.carpet_area AS carpet_area,\n",
    "                       h.super_area AS super_area,\n",
    "                       h.bathroom AS bathroom,\n",
    "                       h.balcony AS balcony,\n",
    "                       h.current_floor AS current_floor,\n",
    "                       h.total_floors AS total_floors,\n",
    "                       h.bhk AS bhk,\n",
    "                       h.price AS price,\n",
    "                       h.car_parking AS car_parking,\n",
    "                       h.price_x_super_area AS price_x_super_area,\n",
    "                       h.amount AS amount,\n",
    "                       h.transaction AS transaction,\n",
    "                       h.furnishing AS furnishing,\n",
    "                       h.overlooking AS overlooking,\n",
    "                       h.ownership AS ownership,\n",
    "                       h.facing AS facing\n",
    "            \"\"\")\n",
    "            return pd.DataFrame([record.values() for record in result], columns=result.keys())\n",
    "    \n",
    "    def get_edges(self, relationship_type):\n",
    "        with self.driver.session() as session:\n",
    "            result = session.run(f\"\"\"\n",
    "                MATCH (h1:House)-[r:{relationship_type}]->(h2:House)\n",
    "                RETURN h1.house_id AS source, h2.house_id AS target, r.weight AS weight\n",
    "            \"\"\")\n",
    "            return pd.DataFrame([record.values() for record in result], columns=result.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e2aad8d1-89bc-4271-acf0-da470c01f88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Đọc dữ liệu\n",
    "train_df = pd.read_csv('data2/train.csv')\n",
    "test_df = pd.read_csv('data2/test.csv')\n",
    "\n",
    "# Feature engineering: Add Price * Super Area\n",
    "train_df['Price_x_SuperArea'] = train_df['Price'] * train_df['Super Area']\n",
    "test_df['Price_x_SuperArea'] = test_df['Price'] * test_df['Super Area']\n",
    "\n",
    "# Thêm cột index làm house_id\n",
    "train_df['index'] = train_df.index\n",
    "test_df['index'] = test_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "171473b4-607a-41a1-8c06-fd2ba65310f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating House nodes...\n"
     ]
    }
   ],
   "source": [
    "# (Tùy chọn) Giảm số lượng node để kiểm tra\n",
    "# train_df = train_df.head(5000)\n",
    "\n",
    "# Kết nối tới Neo4j\n",
    "neo4j_conn = Neo4jConnection(\"bolt://localhost:7687\", \"neo4j\", \"hqiineo4j\")  # Thay bằng thông tin của bạn\n",
    "\n",
    "# Xóa dữ liệu cũ (nếu cần)\n",
    "neo4j_conn.run_query(\"MATCH (n) DETACH DELETE n\")\n",
    "\n",
    "# Tạo node House\n",
    "print(\"Creating House nodes...\")\n",
    "for _, row in train_df.iterrows():\n",
    "    query = \"\"\"\n",
    "        CREATE (h:House {\n",
    "            house_id: $house_id,\n",
    "            carpet_area: $carpet_area,\n",
    "            super_area: $super_area,\n",
    "            bathroom: $bathroom,\n",
    "            balcony: $balcony,\n",
    "            current_floor: $current_floor,\n",
    "            total_floors: $total_floors,\n",
    "            bhk: $bhk,\n",
    "            price: $price,\n",
    "            car_parking: $car_parking,\n",
    "            price_x_super_area: $price_x_super_area,\n",
    "            amount: $amount,\n",
    "            transaction: $transaction,\n",
    "            furnishing: $furnishing,\n",
    "            overlooking: $overlooking,\n",
    "            society: $society,\n",
    "            ownership: $ownership,\n",
    "            facing: $facing,\n",
    "            location: $location\n",
    "        })\n",
    "    \"\"\"\n",
    "    parameters = {\n",
    "        \"house_id\": int(row['index']),\n",
    "        \"carpet_area\": float(row['Carpet Area']),\n",
    "        \"super_area\": float(row['Super Area']),\n",
    "        \"bathroom\": int(row['Bathroom']),\n",
    "        \"balcony\": int(row['Balcony']),\n",
    "        \"current_floor\": int(row['Current Floor']),\n",
    "        \"total_floors\": int(row['Total Floors']),\n",
    "        \"bhk\": int(row['BHK']),\n",
    "        \"price\": float(row['Price']),\n",
    "        \"car_parking\": int(row['Car Parking']),\n",
    "        \"price_x_super_area\": float(row['Price_x_SuperArea']),\n",
    "        \"amount\": float(row['Amount']),\n",
    "        \"transaction\": row['Transaction'],\n",
    "        \"furnishing\": row['Furnishing'],\n",
    "        \"overlooking\": row['Overlooking'],\n",
    "        \"society\": row['Society'],\n",
    "        \"ownership\": row['Ownership'],\n",
    "        \"facing\": row['Facing'],\n",
    "        \"location\": row['Location']\n",
    "    }\n",
    "    neo4j_conn.run_query(query, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "751b0aa4-8b66-4079-8bf2-d11d602efdbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Society nodes...\n"
     ]
    }
   ],
   "source": [
    "# Tạo node Society\n",
    "print(\"Creating Society nodes...\")\n",
    "societies = train_df['Society'].unique()\n",
    "for society in societies:\n",
    "    query = \"\"\"\n",
    "        MERGE (s:Society {name: $name})\n",
    "    \"\"\"\n",
    "    neo4j_conn.run_query(query, {\"name\": society})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b7937d29-2f87-4634-998e-431dc3adcecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Location nodes...\n"
     ]
    }
   ],
   "source": [
    "# Tạo node Location\n",
    "print(\"Creating Location nodes...\")\n",
    "locations = train_df['Location'].unique()\n",
    "for location in locations:\n",
    "    query = \"\"\"\n",
    "        MERGE (l:Location {name: $name})\n",
    "    \"\"\"\n",
    "    neo4j_conn.run_query(query, {\"name\": location})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6a9a4d0a-b7c3-480d-8f08-3559de996b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating BELONGS_TO_SOCIETY relationships...\n"
     ]
    }
   ],
   "source": [
    "# Tạo relationship BELONGS_TO_SOCIETY\n",
    "print(\"Creating BELONGS_TO_SOCIETY relationships...\")\n",
    "for _, row in train_df.iterrows():\n",
    "    query = \"\"\"\n",
    "        MATCH (h:House {house_id: $house_id})\n",
    "        MATCH (s:Society {name: $society})\n",
    "        CREATE (h)-[:BELONGS_TO_SOCIETY]->(s)\n",
    "    \"\"\"\n",
    "    parameters = {\n",
    "        \"house_id\": int(row['index']),\n",
    "        \"society\": row['Society']\n",
    "    }\n",
    "    neo4j_conn.run_query(query, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "90529778-629d-413c-a503-a4cae9df836a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating BELONGS_TO_LOCATION relationships...\n"
     ]
    }
   ],
   "source": [
    "# Tạo relationship BELONGS_TO_LOCATION\n",
    "print(\"Creating BELONGS_TO_LOCATION relationships...\")\n",
    "for _, row in train_df.iterrows():\n",
    "    query = \"\"\"\n",
    "        MATCH (h:House {house_id: $house_id})\n",
    "        MATCH (l:Location {name: $location})\n",
    "        CREATE (h)-[:BELONGS_TO_LOCATION]->(l)\n",
    "    \"\"\"\n",
    "    parameters = {\n",
    "        \"house_id\": int(row['index']),\n",
    "        \"location\": row['Location']\n",
    "    }\n",
    "    neo4j_conn.run_query(query, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "af16683f-255c-47ee-93e1-dbcdd4524a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating SIMILAR_BHK relationships...\n",
      "Processing BHK=3...\n"
     ]
    },
    {
     "ename": "TransientError",
     "evalue": "{code: Neo.TransientError.General.MemoryPoolOutOfMemoryError} {message: The allocation of an extra 48,0 MiB would use more than the limit 716,8 MiB. Currently using 684,0 MiB. dbms.memory.transaction.total.max threshold reached}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTransientError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[61], line 15\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing BHK=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbhk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m query_similar_bhk \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124m    MATCH (h1:House \u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124mbhk: $bhk})\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124m    MATCH (h2:House \u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124mbhk: $bhk})\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124m    CREATE (h1)-[:SIMILAR_BHK \u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124mweight: 1.0 - price_similarity}]->(h2)\u001b[39m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m---> 15\u001b[0m \u001b[43mneo4j_conn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_similar_bhk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbhk\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbhk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[39], line 18\u001b[0m, in \u001b[0;36mNeo4jConnection.run_query\u001b[1;34m(self, query, parameters)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_query\u001b[39m(\u001b[38;5;28mself\u001b[39m, query, parameters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdriver\u001b[38;5;241m.\u001b[39msession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 18\u001b[0m         \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mG:\\MyProject\\cuda\\Lib\\site-packages\\neo4j\\_sync\\work\\session.py:328\u001b[0m, in \u001b[0;36mSession.run\u001b[1;34m(self, query, parameters, **kwargs)\u001b[0m\n\u001b[0;32m    326\u001b[0m bookmarks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_bookmarks()\n\u001b[0;32m    327\u001b[0m parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(parameters \u001b[38;5;129;01mor\u001b[39;00m {}, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 328\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_auto_result\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimpersonated_user\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault_access_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbookmarks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnotifications_min_severity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnotifications_disabled_classifications\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    337\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_auto_result\n",
      "File \u001b[1;32mG:\\MyProject\\cuda\\Lib\\site-packages\\neo4j\\_sync\\work\\result.py:236\u001b[0m, in \u001b[0;36mResult._run\u001b[1;34m(self, query, parameters, db, imp_user, access_mode, bookmarks, notifications_min_severity, notifications_disabled_classifications)\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pull()\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39msend_all()\n\u001b[1;32m--> 236\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_attach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mG:\\MyProject\\cuda\\Lib\\site-packages\\neo4j\\_sync\\work\\result.py:430\u001b[0m, in \u001b[0;36mResult._attach\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    428\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exhausted \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    429\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_attached \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m--> 430\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mG:\\MyProject\\cuda\\Lib\\site-packages\\neo4j\\_sync\\io\\_common.py:184\u001b[0m, in \u001b[0;36mConnectionErrorHandler.__getattr__.<locals>.outer.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 184\u001b[0m         \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (Neo4jError, ServiceUnavailable, SessionExpired) \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    186\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39miscoroutinefunction(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__on_error)\n",
      "File \u001b[1;32mG:\\MyProject\\cuda\\Lib\\site-packages\\neo4j\\_sync\\io\\_bolt.py:864\u001b[0m, in \u001b[0;36mBolt.fetch_message\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    860\u001b[0m \u001b[38;5;66;03m# Receive exactly one message\u001b[39;00m\n\u001b[0;32m    861\u001b[0m tag, fields \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minbox\u001b[38;5;241m.\u001b[39mpop(\n\u001b[0;32m    862\u001b[0m     hydration_hooks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponses[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mhydration_hooks\n\u001b[0;32m    863\u001b[0m )\n\u001b[1;32m--> 864\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfields\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39midle_since \u001b[38;5;241m=\u001b[39m monotonic()\n\u001b[0;32m    866\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[1;32mG:\\MyProject\\cuda\\Lib\\site-packages\\neo4j\\_sync\\io\\_bolt5.py:500\u001b[0m, in \u001b[0;36mBolt5x0._process_message\u001b[1;34m(self, tag, fields)\u001b[0m\n\u001b[0;32m    498\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_server_state_manager\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbolt_states\u001b[38;5;241m.\u001b[39mFAILED\n\u001b[0;32m    499\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 500\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_failure\u001b[49m\u001b[43m(\u001b[49m\u001b[43msummary_metadata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ServiceUnavailable, DatabaseUnavailable):\n\u001b[0;32m    502\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool:\n",
      "File \u001b[1;32mG:\\MyProject\\cuda\\Lib\\site-packages\\neo4j\\_sync\\io\\_common.py:254\u001b[0m, in \u001b[0;36mResponse.on_failure\u001b[1;34m(self, metadata)\u001b[0m\n\u001b[0;32m    252\u001b[0m handler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandlers\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_summary\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    253\u001b[0m Util\u001b[38;5;241m.\u001b[39mcallback(handler)\n\u001b[1;32m--> 254\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hydrate_error(metadata)\n",
      "\u001b[1;31mTransientError\u001b[0m: {code: Neo.TransientError.General.MemoryPoolOutOfMemoryError} {message: The allocation of an extra 48,0 MiB would use more than the limit 716,8 MiB. Currently using 684,0 MiB. dbms.memory.transaction.total.max threshold reached}"
     ]
    }
   ],
   "source": [
    "# Tạo relationship SIMILAR_BHK (chia nhỏ theo BHK)\n",
    "print(\"Creating SIMILAR_BHK relationships...\")\n",
    "bhk_values = train_df['BHK'].unique()\n",
    "for bhk in bhk_values:\n",
    "    print(f\"Processing BHK={bhk}...\")\n",
    "    query_similar_bhk = \"\"\"\n",
    "        MATCH (h1:House {bhk: $bhk})\n",
    "        MATCH (h2:House {bhk: $bhk})\n",
    "        WHERE h1.house_id < h2.house_id\n",
    "        WITH h1, h2,\n",
    "             abs(h1.amount - h2.amount) / (h1.amount + h2.amount + 0.00001) AS price_similarity\n",
    "        WHERE price_similarity < 0.15\n",
    "        CREATE (h1)-[:SIMILAR_BHK {weight: 1.0 - price_similarity}]->(h2)\n",
    "    \"\"\"\n",
    "    neo4j_conn.run_query(query_similar_bhk, {\"bhk\": int(bhk)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aeab055-5447-4560-b159-608ff7e3bbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo relationship SIMILAR_PRICE (chia nhỏ theo khoảng giá)\n",
    "print(\"Creating SIMILAR_PRICE relationships...\")\n",
    "amount_bins = pd.qcut(train_df['Amount'], q=10, duplicates='drop').cat.categories\n",
    "for i in range(len(amount_bins)):\n",
    "    min_amount = amount_bins[i].left\n",
    "    max_amount = amount_bins[i].right\n",
    "    print(f\"Processing Amount range: {min_amount} to {max_amount}...\")\n",
    "    query_similar_price = \"\"\"\n",
    "        MATCH (h1:House)\n",
    "        MATCH (h2:House)\n",
    "        WHERE h1.amount >= $min_amount AND h1.amount <= $max_amount\n",
    "          AND h2.amount >= $min_amount AND h2.amount <= $max_amount\n",
    "          AND h1.house_id < h2.house_id\n",
    "        WITH h1, h2,\n",
    "             abs(h1.amount - h2.amount) / (h1.amount + h2.amount + 0.00001) AS price_similarity\n",
    "        WHERE price_similarity < 0.1\n",
    "        CREATE (h1)-[:SIMILAR_PRICE {weight: 1.0 - price_similarity}]->(h2)\n",
    "    \"\"\"\n",
    "    neo4j_conn.run_query(query_similar_price, {\"min_amount\": float(min_amount), \"max_amount\": float(max_amount)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893d720a-e462-4bcd-ae5e-c45cd2d7564b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trích xuất dữ liệu từ Neo4j\n",
    "print(\"Extracting data from Neo4j...\")\n",
    "nodes_df = neo4j_conn.get_nodes()\n",
    "similar_bhk_edges = neo4j_conn.get_edges(\"SIMILAR_BHK\")\n",
    "similar_price_edges = neo4j_conn.get_edges(\"SIMILAR_PRICE\")\n",
    "\n",
    "# Đóng kết nối Neo4j\n",
    "neo4j_conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30114862-c3d0-45e0-8e81-9ee27f5f5cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kết hợp các cạnh\n",
    "edges_df = pd.concat([similar_bhk_edges, similar_price_edges], ignore_index=True)\n",
    "\n",
    "# Tạo đặc trưng\n",
    "numerical_cols = ['carpet_area', 'super_area', 'bathroom', 'balcony', 'current_floor', \n",
    "                 'total_floors', 'bhk', 'price', 'car_parking', 'price_x_super_area']\n",
    "categorical_cols = ['transaction', 'furnishing', 'overlooking', 'ownership', 'facing']\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "numerical_features = scaler.fit_transform(nodes_df[numerical_cols])\n",
    "\n",
    "encoded_features = []\n",
    "encoders = {}\n",
    "for col in categorical_cols:\n",
    "    encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "    encoded = encoder.fit_transform(nodes_df[[col]])\n",
    "    encoded_df = pd.DataFrame(encoded, columns=[f\"{col}_{cat}\" for cat in encoder.categories_[0]])\n",
    "    encoded_features.append(encoded_df)\n",
    "    encoders[col] = encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5bb074-5302-418e-85d0-c206058b4568",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.concat([pd.DataFrame(numerical_features, columns=numerical_cols)] + encoded_features, axis=1)\n",
    "y = np.log1p(nodes_df['amount'].values)\n",
    "\n",
    "# Tạo edge_index và edge_weight\n",
    "edge_index = torch.tensor(edges_df[['source', 'target']].values.T, dtype=torch.long)\n",
    "edge_weight = torch.tensor(edges_df['weight'].values, dtype=torch.float)\n",
    "\n",
    "# Tạo dữ liệu cho PyTorch Geometric\n",
    "data = Data(\n",
    "    x=torch.tensor(features.values, dtype=torch.float),\n",
    "    edge_index=edge_index,\n",
    "    edge_attr=edge_weight,\n",
    "    y=torch.tensor(y, dtype=torch.float)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30c8951-b060-4029-8e68-ea7c8124a5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kiểm tra số lượng đỉnh và cạnh\n",
    "print(f\"Graph: {data.num_nodes} nodes, {data.num_edges} edges\")\n",
    "\n",
    "# GNN Model with GATConv\n",
    "class GNNModel(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=64):\n",
    "        super(GNNModel, self).__init__()\n",
    "        self.conv1 = GATConv(input_dim, hidden_dim, heads=2, concat=True)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(hidden_dim * 2)\n",
    "        self.conv2 = GATConv(hidden_dim * 2, 32, heads=2, concat=True)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(32 * 2)\n",
    "        self.conv3 = GATConv(32 * 2, 16, heads=1, concat=False)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(16)\n",
    "        self.fc = torch.nn.Linear(16, 1)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_weight = data.x, data.edge_index, data.edge_attr\n",
    "        x = self.conv1(x, edge_index, edge_attr=edge_weight)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.3, training=self.training)\n",
    "        x = self.conv2(x, edge_index, edge_attr=edge_weight)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x, edge_index, edge_attr=edge_weight)\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc(x)\n",
    "        return x.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d9c404-d36b-4a7e-82b4-301db61f30ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training on CPU\n",
    "device = torch.device('cpu')\n",
    "model = GNNModel(input_dim=features.shape[1]).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10, factor=0.5)\n",
    "\n",
    "data = data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daff2adf-73f0-46fb-b7d4-ac41b2cea721",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "for epoch in range(1500):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    mse_loss = F.mse_loss(out, data.y)\n",
    "    mae_loss = F.l1_loss(out, data.y)\n",
    "    mse_weight = min(0.8, 0.2 + epoch / 2500)\n",
    "    mae_weight = 1 - mse_weight\n",
    "    loss = mse_weight * mse_loss + mae_weight * mae_loss\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step(loss)\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f'Epoch {epoch + 1}, Loss: {loss.item():.4f}, MSE Weight: {mse_weight:.2f}')\n",
    "        with torch.no_grad():\n",
    "            sample_pred = out[:5].cpu().numpy()\n",
    "            sample_true = data.y[:5].cpu().numpy()\n",
    "            print(f\"Sample Predictions: {sample_pred}\")\n",
    "            print(f\"Sample True Values: {sample_true}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cb9235-09fe-46f9-b616-6b4a281ab022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation (dùng tập test từ file)\n",
    "def preprocess_data(df, numerical_cols, categorical_cols, target_col, \n",
    "                   scaler=None, encoders=None, target_encoders=None, target_scaler=None, is_train=True):\n",
    "    df = df.copy()\n",
    "    \n",
    "    target_encode_cols = ['Society', 'Location', 'Overlooking']\n",
    "    \n",
    "    if is_train:\n",
    "        scaler = MinMaxScaler()\n",
    "        df[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n",
    "    else:\n",
    "        df[numerical_cols] = scaler.transform(df[numerical_cols])\n",
    "    \n",
    "    encoded_features = []\n",
    "    if is_train:\n",
    "        encoders = {}\n",
    "        target_encoders = {}\n",
    "        target_scaler = MinMaxScaler()\n",
    "        \n",
    "        one_hot_cols = ['Transaction', 'Furnishing', 'Ownership', 'Facing']\n",
    "        for col in one_hot_cols:\n",
    "            encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "            encoded = encoder.fit_transform(df[[col]])\n",
    "            encoded_df = pd.DataFrame(\n",
    "                encoded, \n",
    "                columns=[f\"{col}_{cat}\" for cat in encoder.categories_[0]]\n",
    "            )\n",
    "            encoded_features.append(encoded_df)\n",
    "            encoders[col] = encoder\n",
    "        \n",
    "        target_encoded = []\n",
    "        for col in target_encode_cols:\n",
    "            mean_target = df.groupby(col)[target_col].mean()\n",
    "            df[f'{col}_encoded'] = df[col].map(mean_target)\n",
    "            target_encoders[col] = mean_target\n",
    "            target_encoded.append(df[[f'{col}_encoded']])\n",
    "        \n",
    "        target_encoded_df = pd.concat(target_encoded, axis=1)\n",
    "        scaled_target_encoded = target_scaler.fit_transform(target_encoded_df)\n",
    "        for i, col in enumerate(target_encode_cols):\n",
    "            df[f'{col}_encoded'] = scaled_target_encoded[:, i]\n",
    "            encoded_features.append(df[[f'{col}_encoded']])\n",
    "    else:\n",
    "        for col in ['Transaction', 'Furnishing', 'Ownership', 'Facing']:\n",
    "            encoded = encoders[col].transform(df[[col]])\n",
    "            encoded_df = pd.DataFrame(\n",
    "                encoded, \n",
    "                columns=[f\"{col}_{cat}\" for cat in encoders[col].categories_[0]]\n",
    "            )\n",
    "            encoded_features.append(encoded_df)\n",
    "        \n",
    "        target_encoded = []\n",
    "        for col in target_encode_cols:\n",
    "            default_value = df[target_col].mean() if target_col in df else 0\n",
    "            df[f'{col}_encoded'] = df[col].map(target_encoders.get(col, pd.Series())).fillna(default_value)\n",
    "            target_encoded.append(df[[f'{col}_encoded']])\n",
    "        \n",
    "        target_encoded_df = pd.concat(target_encoded, axis=1)\n",
    "        scaled_target_encoded = target_scaler.transform(target_encoded_df)\n",
    "        for i, col in enumerate(target_encode_cols):\n",
    "            df[f'{col}_encoded'] = scaled_target_encoded[:, i]\n",
    "            encoded_features.append(df[[f'{col}_encoded']])\n",
    "    \n",
    "    feature_df = pd.concat([df[numerical_cols]] + encoded_features, axis=1)\n",
    "    y = np.log1p(df[target_col].values) if target_col in df else None\n",
    "    \n",
    "    return feature_df, y, scaler, encoders, target_encoders, target_scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc45eb6-1bce-494a-91c2-b3504afd041b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_edges(df, feature_df, k=15, sim_threshold=0.6, key_cols=['Society', 'Location', 'BHK']):\n",
    "    from sklearn.neighbors import NearestNeighbors\n",
    "    features = feature_df.values\n",
    "    knn = NearestNeighbors(n_neighbors=min(k + 1, len(df)), metric='cosine', n_jobs=-1)\n",
    "    knn.fit(features)\n",
    "    distances, indices = knn.kneighbors(features)\n",
    "    \n",
    "    edge_index = []\n",
    "    edge_weight = []\n",
    "    \n",
    "    num_nodes = len(df)\n",
    "    for i in range(len(df)):\n",
    "        for j_idx, dist in zip(indices[i][1:], distances[i][1:]):\n",
    "            if j_idx >= num_nodes:\n",
    "                continue\n",
    "            sim = 1 - dist\n",
    "            if sim > sim_threshold and any(df.iloc[i][col] == df.iloc[j_idx][col] for col in key_cols):\n",
    "                edge_index.append([i, j_idx])\n",
    "                edge_index.append([j_idx, i])\n",
    "                edge_weight.append(sim)\n",
    "                edge_weight.append(sim)\n",
    "    \n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "    edge_weight = torch.tensor(edge_weight, dtype=torch.float)\n",
    "    return edge_index, edge_weight\n",
    "\n",
    "# Preprocess test data\n",
    "numerical_cols_input = ['Carpet Area', 'Super Area', 'Bathroom', 'Balcony', 'Current Floor', \n",
    "                        'Total Floors', 'BHK', 'Price', 'Car Parking', 'Price_x_SuperArea']\n",
    "categorical_cols_input = ['Transaction', 'Furnishing', 'Overlooking', 'Ownership', 'Facing']\n",
    "target_col = 'Amount'\n",
    "\n",
    "test_features, test_y, _, _, _, _ = preprocess_data(\n",
    "    test_df, numerical_cols_input, categorical_cols_input, target_col, \n",
    "    scaler=scaler, encoders=encoders, target_encoders=None, target_scaler=None, is_train=False\n",
    ")\n",
    "\n",
    "# Tạo đồ thị cho test (dùng phương pháp trước đó)\n",
    "test_edge_index, test_edge_weight = create_edges(test_df, test_features, k=15, sim_threshold=0.6)\n",
    "test_data = Data(\n",
    "    x=torch.tensor(test_features.values, dtype=torch.float),\n",
    "    edge_index=test_edge_index,\n",
    "    edge_attr=test_edge_weight,\n",
    "    y=torch.tensor(test_y, dtype=torch.float)\n",
    ").to(device)\n",
    "\n",
    "# Đánh giá mô hình\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pred = model(test_data)\n",
    "    pred = np.expm1(pred.cpu().numpy())\n",
    "    true = np.expm1(test_data.y.cpu().numpy())\n",
    "    \n",
    "    mae = mean_absolute_error(true, pred)\n",
    "    mse = mean_squared_error(true, pred)\n",
    "    r2 = r2_score(true, pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mape = np.mean(np.abs((true - pred) / true)) * 100\n",
    "    \n",
    "    print(\"\\nGNN Test Metrics:\")\n",
    "    print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "    print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "    print(f\"R2 Score: {r2:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.2f}\")\n",
    "    print(f\"MAPE: {mape:.2f}%\")\n",
    "\n",
    "# Save model\n",
    "torch.save(model.state_dict(), 'gnn_model_with_neo4j_fixed_memory.pth')\n",
    "\n",
    "# Clear memory\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04343f5e-38e1-4ce2-8a52-3ff0487b014d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda-gpt",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
